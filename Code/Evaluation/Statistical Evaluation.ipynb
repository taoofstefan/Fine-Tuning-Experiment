{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b227784-cb70-457d-8e81-fdee6ef1ad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\stefa\\anaconda3\\envs\\datascience\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\stefa\\anaconda3\\envs\\datascience\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/11.0 MB 9.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/11.0 MB 13.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.8/11.0 MB 16.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.3/11.0 MB 20.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 24.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.3/11.0 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/11.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 34.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB ? eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efaf560-11d2-487f-8e8c-c1cf1f224b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: Mean = 62.8 , Median = 87.0 , Std Dev = 34.20175434096912\n",
      "Baseline Precision: Mean = 12.4 , Median = 5.0 , Std Dev = 13.365627557282899\n",
      "Baseline TCE: Mean = 12.4 , Median = 15.0 , Std Dev = 6.118823416311341\n",
      "Post FT Accuracy: Mean = 86.4 , Median = 87.0 , Std Dev = 5.986651818838307\n",
      "Post FT Precision: Mean = 29.4 , Median = 31.0 , Std Dev = 14.59588983241515\n",
      "Post FT TCE: Mean = 10.0 , Median = 9.0 , Std Dev = 4.381780460041329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Data preparation\n",
    "baseline_data = {\n",
    "    'Model': ['Llama 2', 'Mistral Instruct', 'gpt-3.5-turbo-0125', 'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13'],\n",
    "    'Accuracy': [21, 21, 87, 91, 94],\n",
    "    'Precision': [0, 0, 5, 26, 31],\n",
    "    'TCE': [4, 21, 7, 15, 15]\n",
    "}\n",
    "\n",
    "post_ft_data = {\n",
    "    'Model': ['Llama 2 FT', 'Mistral Instruct FT', 'gpt-3.5-turbo-0125', 'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13'],\n",
    "    'Accuracy': [77, 83, 87, 91, 94],\n",
    "    'Precision': [35, 50, 5, 26, 31],\n",
    "    'TCE': [9, 4, 7, 15, 15]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "baseline_df = pd.DataFrame(baseline_data)\n",
    "post_ft_df = pd.DataFrame(post_ft_data)\n",
    "\n",
    "# Calculate summary statistics\n",
    "def calculate_summary_statistics(df, metric):\n",
    "    mean = np.mean(df[metric])\n",
    "    median = np.median(df[metric])\n",
    "    std_dev = np.std(df[metric])\n",
    "    return mean, median, std_dev\n",
    "\n",
    "# Baseline statistics\n",
    "baseline_accuracy_stats = calculate_summary_statistics(baseline_df, 'Accuracy')\n",
    "baseline_precision_stats = calculate_summary_statistics(baseline_df, 'Precision')\n",
    "baseline_tce_stats = calculate_summary_statistics(baseline_df, 'TCE')\n",
    "\n",
    "# Post FT statistics\n",
    "post_ft_accuracy_stats = calculate_summary_statistics(post_ft_df, 'Accuracy')\n",
    "post_ft_precision_stats = calculate_summary_statistics(post_ft_df, 'Precision')\n",
    "post_ft_tce_stats = calculate_summary_statistics(post_ft_df, 'TCE')\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Baseline Accuracy: Mean =\", baseline_accuracy_stats[0], \", Median =\", baseline_accuracy_stats[1], \", Std Dev =\", baseline_accuracy_stats[2])\n",
    "print(\"Baseline Precision: Mean =\", baseline_precision_stats[0], \", Median =\", baseline_precision_stats[1], \", Std Dev =\", baseline_precision_stats[2])\n",
    "print(\"Baseline TCE: Mean =\", baseline_tce_stats[0], \", Median =\", baseline_tce_stats[1], \", Std Dev =\", baseline_tce_stats[2])\n",
    "\n",
    "print(\"Post FT Accuracy: Mean =\", post_ft_accuracy_stats[0], \", Median =\", post_ft_accuracy_stats[1], \", Std Dev =\", post_ft_accuracy_stats[2])\n",
    "print(\"Post FT Precision: Mean =\", post_ft_precision_stats[0], \", Median =\", post_ft_precision_stats[1], \", Std Dev =\", post_ft_precision_stats[2])\n",
    "print(\"Post FT TCE: Mean =\", post_ft_tce_stats[0], \", Median =\", post_ft_tce_stats[1], \", Std Dev =\", post_ft_tce_stats[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8431bc85-51da-48e1-96cd-bbfc6d09cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Accuracy Change  Precision Change  TCE Change\n",
      "0           Llama 2               56                35           5\n",
      "1  Mistral Instruct               62                50         -17\n"
     ]
    }
   ],
   "source": [
    "# Comparative analysis\n",
    "accuracy_change = post_ft_df['Accuracy'] - baseline_df['Accuracy']\n",
    "precision_change = post_ft_df['Precision'] - baseline_df['Precision']\n",
    "tce_change = post_ft_df['TCE'] - baseline_df['TCE']\n",
    "\n",
    "# Create a DataFrame to hold the changes with model names\n",
    "changes_df = pd.DataFrame({\n",
    "    'Model': baseline_df['Model'],\n",
    "    'Accuracy Change': accuracy_change,\n",
    "    'Precision Change': precision_change,\n",
    "    'TCE Change': tce_change\n",
    "})\n",
    "\n",
    "# Print comparative analysis with model names\n",
    "print(changes_df.iloc[0:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9f8034b-dae5-4481-a162-a9644c40bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Shapiro-Wilk test: ShapiroResult(statistic=0.7138381228579564, pvalue=0.013290752758146148)\n",
      "Precision Shapiro-Wilk test: ShapiroResult(statistic=0.7619236826069935, pvalue=0.038235798010029756)\n",
      "TCE Shapiro-Wilk test: ShapiroResult(statistic=0.7496660163102402, pvalue=0.029551876952296566)\n",
      "Accuracy Test: WilcoxonResult(statistic=0.0, pvalue=0.17971249487899976)\n",
      "Precision Test: WilcoxonResult(statistic=0.0, pvalue=0.17971249487899976)\n",
      "TCE Test: WilcoxonResult(statistic=1.0, pvalue=0.6547208460185769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\anaconda3\\envs\\datascience\\Lib\\site-packages\\scipy\\stats\\_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Baseline and post-fine-tuning data\n",
    "baseline_data = {\n",
    "    'Model': ['Llama 2', 'Mistral Instruct', 'gpt-3.5-turbo-0125', 'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13'],\n",
    "    'Accuracy': [21, 21, 87, 91, 94],\n",
    "    'Precision': [0, 0, 5, 26, 31],\n",
    "    'TCE': [4, 21, 7, 15, 15]\n",
    "}\n",
    "\n",
    "post_ft_data = {\n",
    "    'Model': ['Llama 2 FT', 'Mistral Instruct FT', 'gpt-3.5-turbo-0125', 'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13'],\n",
    "    'Accuracy': [77, 83, 87, 91, 94],\n",
    "    'Precision': [35, 50, 5, 26, 31],\n",
    "    'TCE': [9, 4, 7, 15, 15]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "baseline_df = pd.DataFrame(baseline_data)\n",
    "post_ft_df = pd.DataFrame(post_ft_data)\n",
    "\n",
    "# Calculate the differences\n",
    "accuracy_diff = post_ft_df['Accuracy'] - baseline_df['Accuracy']\n",
    "precision_diff = post_ft_df['Precision'] - baseline_df['Precision']\n",
    "tce_diff = post_ft_df['TCE'] - baseline_df['TCE']\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "accuracy_normality = stats.shapiro(accuracy_diff)\n",
    "precision_normality = stats.shapiro(precision_diff)\n",
    "tce_normality = stats.shapiro(tce_diff)\n",
    "\n",
    "print(\"Accuracy Shapiro-Wilk test:\", accuracy_normality)\n",
    "print(\"Precision Shapiro-Wilk test:\", precision_normality)\n",
    "print(\"TCE Shapiro-Wilk test:\", tce_normality)\n",
    "\n",
    "# Choose the test based on normality\n",
    "if accuracy_normality.pvalue > 0.05:\n",
    "    accuracy_test = stats.ttest_rel(baseline_df['Accuracy'], post_ft_df['Accuracy'])\n",
    "else:\n",
    "    accuracy_test = stats.wilcoxon(baseline_df['Accuracy'], post_ft_df['Accuracy'])\n",
    "\n",
    "if precision_normality.pvalue > 0.05:\n",
    "    precision_test = stats.ttest_rel(baseline_df['Precision'], post_ft_df['Precision'])\n",
    "else:\n",
    "    precision_test = stats.wilcoxon(baseline_df['Precision'], post_ft_df['Precision'])\n",
    "\n",
    "if tce_normality.pvalue > 0.05:\n",
    "    tce_test = stats.ttest_rel(baseline_df['TCE'], post_ft_df['TCE'])\n",
    "else:\n",
    "    tce_test = stats.wilcoxon(baseline_df['TCE'], post_ft_df['TCE'])\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy Test:\", accuracy_test)\n",
    "print(\"Precision Test:\", precision_test)\n",
    "print(\"TCE Test:\", tce_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b573828d-f58d-47b3-837d-0fc1bfbf97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size for Accuracy: 0.7287283470634874\n",
      "Effect Size for Precision: 0.7120516440958823\n",
      "Effect Size for TCE: -0.2842277497795395\n",
      "Effect Size for Accuracy: 0.7287283470634874\n",
      "Effect Size for Precision: 0.7120516440958823\n",
      "Effect Size for TCE: -0.2842277497795395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your baseline and post fine-tuning data\n",
    "baseline_df = pd.DataFrame({\n",
    "    'Model': ['Llama 2', 'Mistral Instruct', 'gpt-3.5-turbo-0125', 'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13'],\n",
    "    'Accuracy': [21, 21, 87, 91, 94],\n",
    "    'Precision': [0, 0, 5, 26, 31],\n",
    "    'TCE': [4, 21, 7, 15, 15]\n",
    "})\n",
    "\n",
    "post_ft_df = pd.DataFrame({\n",
    "    'Model': ['Llama 2 FT', 'Mistral Instruct FT', 'gpt-3.5-turbo-0125', 'gpt-4-turbo-2024-04-09', 'gpt-4o-2024-05-13'],\n",
    "    'Accuracy': [77, 83, 87, 91, 94],\n",
    "    'Precision': [35, 50, 5, 26, 31],\n",
    "    'TCE': [9, 4, 7, 15, 15]\n",
    "})\n",
    "\n",
    "# Function to calculate Cohen's d for paired samples\n",
    "def cohen_d(x, y):\n",
    "    diff = x - y\n",
    "    return np.mean(diff) / np.std(diff, ddof=1)\n",
    "\n",
    "# Calculate effect sizes for each metric\n",
    "accuracy_effect_size = cohen_d(post_ft_df['Accuracy'], baseline_df['Accuracy'])\n",
    "precision_effect_size = cohen_d(post_ft_df['Precision'], baseline_df['Precision'])\n",
    "tce_effect_size = cohen_d(post_ft_df['TCE'], baseline_df['TCE'])\n",
    "\n",
    "print(\"Effect Size for Accuracy:\", accuracy_effect_size)\n",
    "print(\"Effect Size for Precision:\", precision_effect_size)\n",
    "print(\"Effect Size for TCE:\", tce_effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4507316d-c24b-4846-857c-7ffb831d0d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap p-value for Accuracy: 0.1944\n",
      "Bootstrap p-value for Precision: 0.1068\n",
      "Bootstrap p-value for TCE: 0.4945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Function to calculate bootstrap p-values\n",
    "def bootstrap_pvalue(data1, data2, n_resamples=10000):\n",
    "    observed_diff = np.mean(data1) - np.mean(data2)\n",
    "    combined = np.concatenate([data1, data2])\n",
    "    count = 0\n",
    "    for _ in range(n_resamples):\n",
    "        resampled = resample(combined, replace=True, n_samples=len(combined))\n",
    "        resampled_diff = np.mean(resampled[:len(data1)]) - np.mean(resampled[len(data1):])\n",
    "        if abs(resampled_diff) >= abs(observed_diff):\n",
    "            count += 1\n",
    "    p_value = count / n_resamples\n",
    "    return p_value\n",
    "\n",
    "# Bootstrap p-values\n",
    "accuracy_pvalue_bootstrap = bootstrap_pvalue(post_ft_df['Accuracy'], baseline_df['Accuracy'])\n",
    "precision_pvalue_bootstrap = bootstrap_pvalue(post_ft_df['Precision'], baseline_df['Precision'])\n",
    "tce_pvalue_bootstrap = bootstrap_pvalue(post_ft_df['TCE'], baseline_df['TCE'])\n",
    "\n",
    "print(\"Bootstrap p-value for Accuracy:\", accuracy_pvalue_bootstrap)\n",
    "print(\"Bootstrap p-value for Precision:\", precision_pvalue_bootstrap)\n",
    "print(\"Bootstrap p-value for TCE:\", tce_pvalue_bootstrap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81906cca-4a26-4621-bece-93433263774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Cohens d Calculations\n",
      "Metric     Mean Change  Pooled Std Dev  Cohens d   Interpretation\n",
      "Accuracy   23.60        24.27           0.97       Large effect\n",
      "Precision  17.00        15.59           1.09       Large effect\n",
      "TCE        -2.40        5.79            -0.41      Medium effect\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the baseline and post fine-tuning statistics\n",
    "metrics = {\n",
    "    'Accuracy': {'baseline_mean': 62.8, 'post_ft_mean': 86.4, 'baseline_sd': 33.71, 'post_ft_sd': 6.48},\n",
    "    'Precision': {'baseline_mean': 12.4, 'post_ft_mean': 29.4, 'baseline_sd': 14.66, 'post_ft_sd': 16.47},\n",
    "    'TCE': {'baseline_mean': 12.4, 'post_ft_mean': 10.0, 'baseline_sd': 6.51, 'post_ft_sd': 4.96}\n",
    "}\n",
    "\n",
    "# Function to calculate Cohen's d\n",
    "def cohen_d(baseline_mean, post_ft_mean, baseline_sd, post_ft_sd):\n",
    "    mean_diff = post_ft_mean - baseline_mean\n",
    "    pooled_sd = math.sqrt((baseline_sd ** 2 + post_ft_sd ** 2) / 2)\n",
    "    d = mean_diff / pooled_sd\n",
    "    return mean_diff, pooled_sd, d\n",
    "\n",
    "# Calculate and print Cohen's d for each metric\n",
    "results = {}\n",
    "for metric, values in metrics.items():\n",
    "    mean_diff, pooled_sd, d = cohen_d(values['baseline_mean'], values['post_ft_mean'], values['baseline_sd'], values['post_ft_sd'])\n",
    "    results[metric] = {'Mean Change': mean_diff, 'Pooled Std Dev': pooled_sd, 'Cohens d': d}\n",
    "\n",
    "# Print the results\n",
    "print(\"Summary of Cohens d Calculations\")\n",
    "print(f\"{'Metric':<10} {'Mean Change':<12} {'Pooled Std Dev':<15} {'Cohens d':<10} {'Interpretation'}\")\n",
    "for metric, res in results.items():\n",
    "    interpretation = \"Small effect\" if abs(res['Cohens d']) < 0.2 else \\\n",
    "                     \"Medium effect\" if abs(res['Cohens d']) < 0.5 else \\\n",
    "                     \"Large effect\"\n",
    "    print(f\"{metric:<10} {res['Mean Change']:<12.2f} {res['Pooled Std Dev']:<15.2f} {res['Cohens d']:<10.2f} {interpretation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
